# Phase 0 : Backend Set Up

Set up includes:

1. Creating basic project structure.
2. Installing dependencies.
3. Setting up Diesel.
4. Initializing Axum.
5. Connecting security middlewares.
6. Testing your server setup.

## Create a project folder and a README for your project

Using cargo new, create a project directory with the name of your project.
```shell
cargo new $project_name
```

Within the root of your new project directory, create a file called `README.md`.
```shell
touch README.md
```

The `README.md` will include the API documentation and the database schema.

GitHub will utilize this file to be displayed on the main page of your repository on GitHub.

Learn more about [GitHub READMEs here][github-readmes].

Create an `images` folder at the root of your project directory.
```shell
mkdir images
```

Later we will create an image of the schema for documenting our structure to other users.

Add the following to your README.md replacing `[title]` with the name of your project:


```markdown
# [title]

## Database Schema Design

![db-schema]

[db-schema]: ./images/schema.png

## API Documentation
```

[github-readmes]: https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-readmes





## `.gitignore`

Create a `.gitignore` file at the root of the project with the following
contents:

```plaintext
.env
```

## Git

Make sure your local machine's Git configuration initializes new Git repositories
with a default branch of `main` by running:

```bash
git config --global init.defaultBranch main
```

Initiate Git in the project folder by running the following command in the
root directory of your project folder:

```bash
git init
```

[Create a public remote git repository in GitHub][new-git-repo] with the same name as your project folder.


**Note**: Make sure the GitHub repo that you create is "Public", NOT "Private".

Connect your remote repository to your local repository by running the following
command in the root directory:

```bash
git remote add origin <github-remote-url>
```

Replace `github-remote-url` with the URL that was generated by
GitHub after creating the public remote GitHub repository.

Make your first commit and push it to the remote repository.

```bash
git add .
git status
```

Confirm that the files to be added include the `README.md` and `.gitignore` files
that you created.

Commit the changes:

```bash
git commit -m 'Initial commit'
```

Push the commit to the GitHub remote repository:

```bash
git push origin main
```

Confirm that you see the `README.md` and `.gitignore` file on GitHub.

After pushing to GitHub, the `README.md` file should now be rendered as the
repository's main page.

## Backend and Frontend Separation

In this project, you will separate the backend Axum code from the frontend
React code.

Inside of the root directory of your project folder, create two folders called
`backend` and `frontend`.

Your file structure should look like this:

```plaintext
.
├── backend/
├── frontend/
├── images/
├── .gitignore
└── README.md
```

## Dependencies

In the `Cargo.toml` file and add the following dependencies:

```toml
[dependencies]
pq-sys = "0.6.0"                                                            # Rust binding to the PostgreSQL library
axum = "0.8.1"                                                              # Server
axum-macros = "0.5.0"                                                       # Server macros
diesel = { version = "2.2.8", features = ["postgres", "chrono", "r2d2"] }   # ORM
dotenvy = "0.15.7"                                                          # For getting environment variables

serde = { version = "1.0.219", features = ["derive"] }
serde_json = "1.0.140"

tokio = { version = "1.44.1", features = ["full"] }

cookie = "0.18.1"
csrf = "0.5.0"                                                              # CSRF protection
jsonwebtoken = "9.3.1"                                                      # JWT implementation

tower = "0.4"

tower-http = { version = "0.6.2", features = [                              # provides HTTP-specific middleware and utilities built on top of tower
    "catch-panic",                                                          # Convert panics into responses.
    "cors",                                                                 # CORS middleware
    "set-header",                                                           # Security headers middleware (helmet equivalent)
    "set-status",                                                           # Middleware to override status codes.
    "trace",                                                                # Middleware for HTTP request/response logging
    "validate-request",                                                     # Middleware that validates requests.
] }


tracing = "0.1.41"                                                          # Logging framework
tracing-subscriber = "0.3.19"                                               # Logging implementation

config = "0.13"                                                             # Configuration management with environment support
thiserror = "2.0.12"                                                        # Error handling
chrono = { version = "0.4", features = ["serde"] }


[dev-dependencies]
cargo-watch = "8.5.3"                                                       # Equivalent to nodemon

```

From the root do a cargo run to compile the dependencies:

```shell
cargo run
```

## Create Postgres Database

In terminal log in to psql.

Then run:

```sql
CREATE DATABASE your_database_name
```

## Configuration

In the `backend` folder, create a `.env` to define your environment variables.

Populate the `.env` file based on the example below:

```plaintext
SERVER_PORT=5678
DATABASE_URL=postgres://postgres:postgres@localhost/«your_database_name_here»
DB_PORT=5432
DB_FILE=db/dev.db
JWT_SECRET=«generate_strong_secret_here»
JWT_EXPIRES_IN=604800
SCHEMA=«custom_schema_name_here»
RUST_LOG=debug
```

Assign `DB_PORT` to `5432`, choose a custom schema name in snake case, and generate a strong JWT secret.

> Recommendation to generate a strong secret: <br>
>  - Create a random string using `openssl`. <br>
>  - `openssl` should already be installed in your Ubuntu/MacOS shell. <br>
>  - To generate a random JWT secret run:
> ```shell
>    openssl rand -base64 10
> ```
> - This generates a pseudo randomized string of 10 characters.
> - Copy and paste the generated string to your .env file. <br>
> <br>

Next create a configuration file to read the environment variables and export them.

Add a folder called `config` in your `backend` folder.
```shell
mkdir config
```

Inside the `config` folder, create an `config.rs` file with the following contents:

```rust
use serde::Deserialize;
use std::env::{self, VarError};
use thiserror::Error;

#[derive(Error, Debug)]
pub enum ConfigError {
    #[error("Environment variable error: {0}")]
    EnvVar(#[from] VarError),
    #[error("{0}")]
    Other(String),
}

#[derive(Debug, Deserialize)]
pub struct Config {
    pub database_url: String,
    pub jwt_secret: String,
    pub rust_log: String,
    pub jwt_expires_in: String,
    pub schema: String,
}

impl Config {
    pub fn new() -> Result<Self, ConfigError> {
        Ok(Config {
            database_url: env::var("DATABASE_URL")
                .map_err(|_| ConfigError::Other("DATABASE_URL must be set".to_string()))?,

            jwt_secret: env::var("JWT_SECRET")
                .map_err(|_| ConfigError::Other("JWT_SECRET must be set".to_string()))?,

            rust_log: env::var("RUST_LOG")
                .map_err(|_| ConfigError::Other("RUST_LOG must be set".to_string()))?,

            jwt_expires_in: env::var("JWT_EXPIRES_IN")
                .map_err(|_| ConfigError::Other("JWT_EXPIRES_IN must be set".to_string()))?,

            schema: env::var("SCHEMA")
                .map_err(|_| ConfigError::Other("SCHEMA must be set".to_string()))?,
        })
    }
}
```

Each environment variable will be read and exported as a key from this file.


## Diesel Setup

Initialize Diesel in your project by runing this command in the root:

```diesel setup```

This command will:
- Create a migrations directory
- Create a diesel.toml file
- Create the database if it doesn't exist


Create your first migration by running this command in the root:

```shell
diesel migration generate your_table_name
```

This creates two files in migrations/[timestamp]_create_your_table/:
- up.sql: Define your table schema
- down.sql: Define how to revert the migration


Edit the up.sql with your table creation SQL. For example:
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name VARCHAR NOT NULL,
    username VARCHAR NOT NULL,
    email VARCHAR NOT NULL,
    password_hash VARCHAR NOT NULL,
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);
```

Edit the down.sql with your table creation SQL. For example:
```sql
DROP TABLE users;
```


Set up the database connection in a new file src/db.rs:

```rs
use diesel::prelude::*;

pub fn establish_connection(database_url: &str) -> PgConnection {
    PgConnection::establish(database_url)
        .unwrap_or_else(|_| panic!("Error connecting to {}", database_url))
}
```



Run the migration from the root:
```shell
diesel migration run
```

After running the migration, Diesel will:
- Execute your up.sql file
- Create a schema.rs file in your src directory
- Track the migration in a diesel schema migrations table


To verify your migrations, you can run these commands from the root:

- List all migrations and their status
```shell
diesel migration list
```

- Check if any migrations are pending
```shell
diesel migration pending
```

- Run any pending migrations
```shell
diesel migration run
```

- Revert a migration:
```shell
diesel migration revert
```

- Redo a migration:
```shell
diesel migration redo
```

After completing these steps, your project will be set up with Diesel and ready to interact with your database.

The schema.rs file will be automatically generated by Diesel when you run migrations.

You'll need to create your model files to define your database models.

## Models

It is best to create a models directory with an entry file for cleaner import/export. A possible configuration could be:

```
src/
├── models/
│   ├── mod.rs          # Re-exports all models
│   ├── user.rs         # User model
│   ├── post.rs         # Post model
│   └── ...             # Other model files
├── config/
│   ├── mod.rs          # Re-exports all models
│   ├── config.rs       # Configs
│   └── ...             # Other config files
├── db.rs
├── schema.rs
└── main.rs
```

The models/mod.rs file is an entry file for importing/exporting:

```rust
// importing the models
mod user;
mod post;

// re-export models
pub use user::*;
pub use post::*;
```

Then create the user model as follows:

```rust
use diesel::prelude::*;
use serde::{Deserialize, Serialize};
use crate::schema::users;

#[derive(Queryable, Selectable, Deserialize, Serialize)]
#[diesel(table_name = crate::schema::users)]
#[diesel(check_for_backend(diesel::pg::Pg))]
pub struct User {
    pub id: i32,
    pub name: String,
    pub username: String,
    pub email: String,
    pub password_hash: String,
    pub created_at: chrono::NaiveDateTime,
    pub updated_at: chrono::NaiveDateTime,
}

#[derive(Insertable, Deserialize)]
#[diesel(table_name = users)]
pub struct NewUser {
    pub name: String,
    pub username: String,
    pub email: String,
    pub password_hash: String,
}

#[derive(AsChangeset,Deserialize)]
#[diesel(table_name = users)]
pub struct UpdateUser {
    pub name: Option<String>,
    pub username: Option<String>,
    pub email: Option<String>,
    pub password: Option<String>,
}
```

Make sure to add the following to your src/lib.rs or src/main.rs:
```
#[macro_use]
extern crate diesel;

pub mod schema;
pub mod models;
```

## Axum Setup

After you setup Diesel, it's time to start working on getting your Axum
application set up.

### `What Is Axum`

Axum is a web app framework that focuses on modularity.

It does not have its own middleware therefore it uses tower::Service.

Because of this you can share middleware with applications written using hyper or tonic(?).

Axum is compatible to work with tokio as well.



## `main.rs`

The `main.rs` file will be where you initialize your Axum application.

At the top of the file, import the following packages:

```rust
use diesel::prelude::*;
use diesel::r2d2::{self, ConnectionManager, Pool};
use dotenvy::dotenv;
use std::sync::Arc;
use axum::{
    routing::{get, post},
    Router,
    extract::State,
};
use std::net::SocketAddr;

#[macro_use]
extern crate diesel;
pub mod models;
pub mod schema;

mod db;
mod config;

use config::Config;
```

Then, before main, create a struct to represent the state for a web application using a PostgreSQL database connection pool.

```rust
pub struct AppState {
    pub db_pool: Pool<ConnectionManager<PgConnection>>,
}

```

We will also want to creates a type alias for a thread-safe shared reference to AppState using Arc (Atomic Reference Counting).

```rust
type AppStateShare = Arc<AppState>;
```

This provides shared ownership of a value across multiple threads, keeping track of the number of references to the data, and only deallocating the data when the last reference is drop.

# fn main

Before we initialize main we need to add the tokio decorator so we can use main as async:

```rust
#[tokio::main]
async fn main() {
```

Then we need to load environment variables from .env into the program's environment using dotenv() from dotenvy.

We also use .ok() to convert the Result into an Option and ignore any errors that might occur if .env can't be loaded:

```rust
dotenv().ok();
```

Then using ConnectionManager we create a manager with the generic type PgConnection that references the database URL from the config.rs file.

```rust
let manager = ConnectionManager::<PgConnection>::new(&config.database_url);
```

Next we use Pool::builder() to create a connection pool with default settings:

```rust
   let pool = Pool::builder()
        .build(manager)
        .expect("Failed to create pool");
```
This establishes the pool with the databse URL from the manager.

Next we again use ARC to create a thread-safe shared state containing the database connection pool:

```rust
let shared_state = Arc::new(AppState {
    db_pool: pool,
});
```

Next we initialize the tracing subscriber for logging in your application:

```rust
tracing_subscriber::fmt::init();
```

 The tracing system provides structured logging capabilities that are more powerful than simple println statements.

 Here is a basic usage example of tracing:

 ```rust
 use tracing::{info, warn, error, debug};

// Initialize the default subscriber
tracing_subscriber::fmt::init();

// Now you can use logging macros
info!("Application started");
debug!("Debug information");
warn!("Warning message");
error!("Error occurred: {}", error_message);
```

This increases the capability of logging for example you can customize the logging format and options:

```rust
use tracing_subscriber::fmt::format::FmtSpan;

// More customized initialization
tracing_subscriber::fmt()
    .with_target(false) // Don't include targets in output
    .with_thread_ids(true) // Include thread IDs
    .with_level(true) // Include log levels
    .with_file(true) // Include file path
    .with_line_number(true) // Include line numbers
    .with_thread_names(true) // Include thread names
    .with_span_events(FmtSpan::FULL) // Log all span events
    .json() // Use JSON format
    .init();
```
The logging levels in order of verbosity:

- ERROR: Error conditions

- WARN: Warning messages

- INFO: General information

- DEBUG: Detailed information for debugging

- TRACE: Very detailed debugging information

You can control the log level using the RUST_LOG environment variable:

```rust
RUST_LOG=debug cargo run
# or
RUST_LOG=info,my_crate=debug cargo run
```

Now in fn main we create a new Axum router and configure ti with routes and shared state:

```rust
let app = Router::new()
    .route("/", get(root))
    .route("/health", get(health_check))
    .with_state(shared_state);
```

This router has temporary handlers "root" and "health_check" for the corresponding routes. Handlers will live in different folders and be imported so as to follow the Single Responsility Principle.

Now we must create the listening address for the server which will be a socket address that includes the IP and port:

```rust
let addr = SocketAddr::from(([127, 0, 0, 1], 5678));
println!("Server running on http://{}", addr);
```

Finally we create a TCP listener and start the Axum web server:

```rust
// Create TCP listener bound to our address
let listener = tokio::net::TcpListener::bind(addr).await.unwrap();

// Start serving the application
axum::serve(listener, app.into_make_service())
    .await
    .unwrap();
}
```

Don't forget the closing bracket to close the main function.

The server should run and listen using the socket it was assigned to:

```rust
cargo run
```

Ignore any warnings for now and open the browser to the socket address:

```
http://127.0.0.1:5678
```
You should see "Hello, World!" in the browser!


## `Middleware`

### `Cookies`

There are a few strategies for handling cookies.

1. functionality built in to Axum

    - The official extension from the Axum team, is minimal and works well for simple setups.

2. cookie Crate

    - For more complex cookies

3. tower-cookies

    - A middleware-based cookie manager for Axum & Tower, with easy read/write access and no manual jar passing.

We'll cover each approach but recommend tower-cookies.
<br>

#### Axum For Cookies

In Axum, cookie handling is built into the framework.

In main.rs modify the axum import with the axum::extract::Cookie extractor and axum::response::AppendHeaders for setting cookies.

You don't need a separate middleware like Express's cookie-parser.

But you will need to create handlers and add them to the routes:

```rust

    // Build our application with routes
    let app = Router::new()
        .route("/", get(root))
        .route("/health", get(health_check))
        .route("/set-cookie", get(set_cookie))
        .route("/read-cookie", get(read_cookie))
        .with_state(shared_state);



// New cookie handlers
async fn set_cookie() -> impl IntoResponse {
    AppendHeaders([
        ("Set-Cookie", "session=abc123; HttpOnly; Path=/")
    ])
}

async fn read_cookie(cookie: Option<Cookie>) -> impl IntoResponse {
    match cookie {
        Some(cookie) => format!("Cookie value: {}", cookie.value()),
        None => "No cookie found".to_string(),
    }
}
```

Here's how your new main.rs should look:

```rust
use diesel::prelude::*;
use diesel::r2d2::{self, ConnectionManager, Pool};
use dotenvy::dotenv;
use std::sync::Arc;

mod db;
mod config;

use config::Config;

use axum::{
    routing::{get, post},
    Router,
    extract::{State, Cookie},
    response::{AppendHeaders, IntoResponse},
};
use std::net::SocketAddr;

#[macro_use]
extern crate diesel;
pub mod models;
pub mod schema;

// Define AppState to hold shared state
pub struct AppState {
    pub db_pool: Pool<ConnectionManager<PgConnection>>,
}

// Create a type alias for convenience
type AppStateShare = Arc<AppState>;

#[tokio::main]
async fn main() {
    // Load .env file
    dotenv().ok();

    // Access environment variables
    let config: Config = Config::new().expect("Failed to load configuration");

    // Set up connection pool
    let manager = ConnectionManager::<PgConnection>::new(&config.database_url);
    let pool = Pool::builder()
        .build(manager)
        .expect("Failed to create pool");

    // Create shared state
    let shared_state = Arc::new(AppState {
        db_pool: pool,
    });

    // Initialize tracing for logging
    tracing_subscriber::fmt::init();

    // Build our application with routes
    let app = Router::new()
        .route("/", get(root))
        .route("/health", get(health_check))
        .route("/set-cookie", get(set_cookie))
        .route("/read-cookie", get(read_cookie))
        .with_state(shared_state);

    // Run the server
    let addr = SocketAddr::from(([127, 0, 0, 1], 5678));
    println!("Server running on http://{}", addr);

    let listener = tokio::net::TcpListener::bind(addr).await.unwrap();
    axum::serve(listener, app.into_make_service())
        .await
        .unwrap();
}

// Example of a handler using state
async fn root(
    State(state): State<AppStateShare>,
    cookie: Option<Cookie>,
) -> impl IntoResponse {
    // You can now access both state and cookies
    "Hello, World!"
}

// Health check endpoint
async fn health_check() -> &'static str {
    "OK"
}

// New cookie handlers
async fn set_cookie() -> impl IntoResponse {
    AppendHeaders([
        ("Set-Cookie", "session=abc123; HttpOnly; Path=/")
    ])
}

async fn read_cookie(cookie: Option<Cookie>) -> impl IntoResponse {
    match cookie {
        Some(cookie) => format!("Cookie value: {}", cookie.value()),
        None => "No cookie found".to_string(),
    }
}

```
<br>

#### cookie crate For Cookies


For more complex cookie operations, you can use the cookie crate (which you already have in your dependencies). Here's an example:

```rust
use cookie::{Cookie as RawCookie, SameSite};

async fn set_complex_cookie() -> impl IntoResponse {
    let cookie = RawCookie::build("session", "value123")
        .path("/")
        .secure(true)
        .http_only(true)
        .same_site(SameSite::Strict)
        .finish();

    AppendHeaders([
        ("Set-Cookie", cookie.to_string())
    ])
}
```
<br>

#### tower-cookies For Cookies

You can also use tower-cookies which provides a more streamlined approach.

First add the latest tower-cookies dependency to the cargo.toml:

```shell
cargo add tower-cookies
```

Create a folder called Middleware and then a file called cookies.rs with this code:

```rust
use tower_cookies::{CookieManagerLayer, Cookies, Cookie};
use axum::{response::IntoResponse, Json};
use serde_json::json;

/// Adds a signed cookie
pub fn set_jwt_cookie(cookies: &Cookies, token: &str) {
    let mut cookie = Cookie::new("jwt", token.to_string());
    cookie.set_path("/");
    cookie.set_http_only(true);
    cookie.set_secure(true);

    cookies.add(cookie);
}

/// Retrieves JWT from cookie
pub fn get_jwt_cookie(cookies: &Cookies) -> Option<String> {
    cookies.get("jwt").map(|c| c.value().to_string())
}

/// Example protected route using the jwt cookie
pub async fn protected_route(cookies: Cookies) -> impl IntoResponse {
    match get_jwt_cookie(&cookies) {
        Some(token) => Json(json!({
            "success": true,
            "token": token
        }))
        .into_response(),

        None => Json(json!({
            "success": false,
            "message": "Unauthorized"
        }))
        .into_response(),
    }
}

/// Expose cookie middleware layer
pub fn cookie_layer() -> CookieManagerLayer {
    CookieManagerLayer::new()
}
```
In the Middleware folder add a mod.rs and add the import/export:

```rust
pub mod cookies;
```

Then in main.rs add the import:

```rust
use middleware::cookies::{cookie_layer, protected_route};
```
Then in main.rs add the handlers to the router:

```rust
    let app: Router = Router::new()
        .route("/", get(root))
        .route("/health", get(health_check))
        .route("/protected", get(protected_route))
        .with_state(shared_state)
        .layer(cookie_layer())
```
<br>

#### Comparing Approaches For Cookies

This last approach is recommended because it manages some cookie operations automatically:

```text
    - No need to pass `CookieJar`around in handlers — cookies are injected via extractor (`Cookies`).
    - More convenient and ergonomic** for real-world apps (especially with multiple layers).
    - Supports signed and private cookies with `Key`.
    - Inspired by cookies in Express.js and Koa (intuitive for web devs coming from those).
    - More middleware-friendly (cookies persist across all handlers/middleware automatically).
```

Here is a breakdown comparison of the approaches:

```text
    | Use Case                               | Use `tower-cookies` ✅ | Use `axum-extra::CookieJar` ✅|
    |----------------------------------------|------------------------|-------------------------------|
    | Full app with sessions/auth            | ✅ Yes                 | Maybe                         |
    | Want ergonomic read/write cookies      | ✅ Yes                 | 👎 Manual                     |
    | Only need simple read/set              | 👎 Overkill            | ✅ Yes                        |
    | Stateless APIs, cookie just for flavor | 👎                     | ✅ Yes                        |
    | Already using Axum extra extractors    | Maybe                  | ✅                            |
    | Want auto cookie injection             | ✅ Yes                 | 👎                            |

```
<br>

## `JSON`

In Axum, JSON handling is built-in through extractors.

Add Json to the Axum import in main.rs:

```rust
use axum::{
    routing::{post, get},
    Json,
    Router,
};
```

Use the Json extractor directly in your route handlers.

Here's an example how to handle JSON:

```rust
// Handler that receives JSON
async fn create_user(
    Json(payload): Json<User>
) -> impl IntoResponse {
    // payload is now a User struct
    println!("Received user: {} {}", payload.name, payload.email);

    // Return JSON response
    Json(User {
        name: payload.name,
        email: payload.email,
    })
}

// In your router setup:
let app = Router::new()
    .route("/users", post(create_user));

// Shell
curl -X POST -H "Content-Type: application/json" -d '{"name":"John","email":"john@example.com"}' http://localhost:5678/users

```



## `CORS`

In Axum, CORS is handled through the tower-http crate's CorsLayer.

We only allow CORS (Cross-Origin Resource Sharing) in development using the
`cors` middleware because the React frontend will be served from a different
server than the Axum server.

CORS isn't needed in production since all of our React and Axum resources will come from the same origin.

Based on the project structure and dependencies, here's how to implement CORS:

```rust
use diesel::prelude::*;
use diesel::r2d2::{self, ConnectionManager, Pool};
use dotenvy::dotenv;
use std::sync::Arc;

mod db;
mod config;

use config::Config;

use axum::{
    routing::{get, post},
    Router,
    extract::State,
    http::{Method, HeaderName, header, HeaderValue},
};
use std::net::SocketAddr;

use tower_http::cors::{CorsLayer, Any};


#[macro_use]
extern crate diesel;
pub mod models;
pub mod schema;

// Define AppState to hold shared state
pub struct AppState {
    pub db_pool: Pool<ConnectionManager<PgConnection>>,
}

// Create a type alias for convenience
type AppStateShare = Arc<AppState>;

#[tokio::main]
async fn main() {
    // Load .env file
    dotenv().ok();

    // Access environment variables
    let config: Config = Config::new().expect("Failed to load configuration");

    // Get environment
    let environment = std::env::var("ENVIRONMENT").unwrap_or_else(|_| "development".to_string());


    println!("environment: {:?}", environment);


    // Configure CORS based on environment
    let cors = if environment == "production" {
        CorsLayer::new()
            .allow_origin("https://your-production-domain.com".parse::<HeaderValue>().unwrap())
            .allow_methods([Method::GET, Method::POST, Method::PATCH, Method::DELETE])
            .allow_headers([
                HeaderName::from_static("content-type"),
                HeaderName::from_static("authorization"),
            ])
            .allow_credentials(true)
    } else {
        // Development CORS settings
        CorsLayer::new()
            .allow_origin("http://127.0.0.1:5678".parse::<HeaderValue>().unwrap())
            .allow_methods([
                Method::GET,
                Method::POST,
                Method::PATCH,
                Method::DELETE,
                Method::OPTIONS,
            ])
            .allow_headers([
                HeaderName::from_static("content-type"),
                HeaderName::from_static("authorization"),
                HeaderName::from_static("accept"),
            ])
            .allow_credentials(true)
    };


    // Set up connection pool
    let manager = ConnectionManager::<PgConnection>::new(&config.database_url);
    let pool = Pool::builder()
        .build(manager)
        .expect("Failed to create pool");

    // Create shared state
    let shared_state = Arc::new(AppState {
        db_pool: pool,
    });

    // Initialize tracing for logging
    tracing_subscriber::fmt::init();

    // Build our application with routes and add CORS
    let app = Router::new()
        .route("/", get(root))
        .route("/health", get(health_check))
        .with_state(shared_state)
        .layer(cors); // Add the CORS middleware here

    // Run the server
    let addr = SocketAddr::from(([127, 0, 0, 1], 5678));
    println!("Server running on http://{}", addr);

    let listener = tokio::net::TcpListener::bind(addr).await.unwrap();
    axum::serve(listener, app.into_make_service())
        .await
        .unwrap();
}

// Example of a handler using state
async fn root(State(state): State<AppStateShare>) -> &'static str {
    // You can now use state.db_pool to get a connection when needed
    "Hello, World!"
}

// Health check endpoint
async fn health_check() -> &'static str {
    "OK"
}
```

We can test these using either fetches in the browser:

```javascript
// Test GET request to root
fetch('http://localhost:5678/', {
  method: 'GET',
  credentials: 'include', // necessary for CORS with credentials
  headers: {
    'Content-Type': 'application/json'
  }
})
.then(response => response.text())
.then(data => console.log(data))
.catch(error => console.error('Error:', error));

// Test health check endpoint
fetch('http://localhost:5678/health', {
  method: 'GET',
  credentials: 'include',
  headers: {
    'Content-Type': 'application/json'
  }
})
.then(response => response.text())
.then(data => console.log(data))
.catch(error => console.error('Error:', error));
```

We can also test this from the command line with curl:

```shell
# Test GET request to root
curl -X GET http://localhost:5678/ \
  -H "Content-Type: application/json" \
  -H "Origin: http://localhost:5173"

# Test health check endpoint
curl -X GET http://localhost:5678/health \
  -H "Content-Type: application/json" \
  -H "Origin: http://localhost:5173"

# Test CORS preflight request
curl -X OPTIONS http://localhost:5678/ \
  -H "Origin: http://localhost:5173" \
  -H "Access-Control-Request-Method: GET" \
  -H "Access-Control-Request-Headers: content-type" \
  -v
```



## `Helmet`




2. Enable better overall security with the `helmet` middleware (for more on what
`helmet` is doing, see [helmet on the `npm` registry]). React is generally safe
at mitigating XSS (i.e., [Cross-Site Scripting]) attacks, but do be sure to
research how to protect your users from such attacks in React when deploying a
large production application. Now add the `crossOriginResourcePolicy` to the
`helmet` middleware with a `policy` of `cross-origin`. This will allow images
with URLs to render in deployment.

## `CSURF`

Add the `csurf` middleware and configure it to use cookies.

```js
// Security Middleware
if (!isProduction) {
  // enable cors only in development
  app.use(cors());
}

// helmet helps set a variety of headers to better secure your app
app.use(
  helmet.crossOriginResourcePolicy({
    policy: "cross-origin"
  })
);

// Set the _csrf token and create req.csrfToken method
app.use(
  csurf({
    cookie: {
      secure: isProduction,
      sameSite: isProduction && "Lax",
      httpOnly: true
    }
  })
);
```

The `csurf` middleware will add a `_csrf` cookie that is HTTP-only (can't be
read by JavaScript) to any server response. It also adds a method on all
requests (`req.csrfToken`) that will be set to another cookie (`XSRF-TOKEN`)
later on. These two cookies work together to provide CSRF (Cross-Site Request
Forgery) protection for your application. The `XSRF-TOKEN` cookie value needs to
be sent in the header of any request with all HTTP verbs besides `GET`. This
header will be used to validate the `_csrf` cookie to confirm that the
request comes from your site and not an unauthorized site.

Now that you set up all the pre-request middleware, it's time to set up the
routes for your Axum application.

### Routes

Create a folder called `routes` in your `backend` folder. All your routes will
live in this folder.

Create an `index.js` file in the `routes` folder. In this file, create an
Axum router, create a test route, and export the router at the bottom of the
file.

```js
// backend/routes/index.js
const Axum = require('Axum');
const router = Axum.Router();

router.get('/hello/world', function(req, res) {
  res.cookie('XSRF-TOKEN', req.csrfToken());
  res.send('Hello World!');
});

module.exports = router;
```

In this test route, you are setting a cookie on the response with the name of
`XSRF-TOKEN` to the value of the `req.csrfToken` method's return. Then, you are
sending the text, `Hello World!` as the response's body.

Add the routes to the Axum application by importing with the other imports
in `backend/app.js` and connecting the exported router to `app` after all the
middlewares.

```js
// backend/app.js
const routes = require('./routes');

// ...

app.use(routes); // Connect all the routes
```

Finally, at the bottom of the `app.js` file, export `app`.

```js
// backend/app.js
// ...

module.exports = app;
```

After setting up the Axum application, it's time to create the server.

### `bin/www`

Conventionally, the `bin/www` file in an Axum server is the entry file or the
starting point to start the Axum server.

The intention of the `./bin/www` file is for it to be an executable
script, meaning that you could start the application by simply entering the file
name in the terminal as a command:

```sh
bin/www
```

Create a folder in `backend` called `bin`. Inside of it, add a file called
`www` with the following contents:

```js
#!/usr/bin/env node
// backend/bin/www

// Import environment variables
require('dotenv').config();

const { port } = require('../config');

const app = require('../app');
const db = require('../db/models');

// Check the database connection before starting the app
db.Diesel
  .authenticate()
  .then(() => {
    console.log('Database connection success! Diesel is ready to use...');

    // Start listening for connections
    app.listen(port, () => console.log(`Listening on port ${port}...`));
  })
  .catch((err) => {
    console.log('Database connection failure.');
    console.error(err);
  });
```

Here, you will be starting your Axum application to listen for server
requests only after authenticating your database connection.

## Test the Server

At this point, your database, Axum application, and server are all set up and
ready to be tested!

In your `package.json`, add the following scripts:

```json
  "scripts": {
    "Diesel": "Diesel",
    "Diesel-cli": "Diesel-cli",
    "start": "per-env",
    "start:development": "nodemon ./bin/www",
    "start:production": "node ./bin/www",
    "build": "node psql-setup-script.js"
  }
```

`npm start` will run the `/bin/www` in `nodemon` when started in the development
environment with the environment variables in the `.env` file loaded, or in
`node` when started in production.

Now, it's time to finally test your entire set up!

Run `npm start` in the `backend` folder to start your server on the port defined
in the `.env` file, which should be `8000`.

Navigate to the test route at [http://localhost:8000/hello/world]. You should
see the text `Hello World!`. Take a look at your cookies in the `Application`
tab of your Chrome DevTools Inspector. Delete all the cookies to make sure there
are no lingering cookies from other projects, then refresh the page. You should
still see the text `Hello World!` on the page as well as two cookies, one called
`_csrf` and the other called `XSRF-TOKEN` in your DevTools.

If you don't see this, then check your backend server logs in the terminal
where you ran `npm start`. Then check your routes.

If there is a database connection error, make sure you set up the correct
username and password defined in the `.env` file.

When you're finished testing, commit! Now is a good time to commit because you
have working code.

## CSRF Token access for development

You can now remove the `GET /hello/world` test route.

Add a route, `GET /api/csrf/restore` to allow any developer to re-set the CSRF
token cookie `XSRF-TOKEN`.

In this route, you are setting a cookie on the response with the
name of `XSRF-TOKEN` to the value of the `req.csrfToken` method's return. Then,
send the token as the response for easy retrieval.

Add this route to the `backend/routes/index.js` file.

```js
// backend/routes/index.js
// ...
// Add a XSRF-TOKEN cookie
router.get("/api/csrf/restore", (req, res) => {
  const csrfToken = req.csrfToken();
  res.cookie("XSRF-TOKEN", csrfToken);
  res.status(200).json({
    'XSRF-Token': csrfToken
  });
});
// ...
```

This route should not be available in production, but it will not be exclusive
to the production application until you implement the frontend of the
application later. So for now, it will remain available to both the development
and production environments.

## Commit your code

Now is a good time to commit and push your code to GitHub!

Here's a recommendation for what to write as your commit message:
"Initialize Axum and Diesel with CSRF protection"

[new-git-repo]: https://github.com/new
[helmet on the `npm` registry]: https://www.npmjs.com/package/helmet
[Axum error-handling middleware]: https://Axumjs.com/en/guide/using-middleware.html#middleware.error-handling
[model-level validations]: https://Diesel.org/master/manual/validations-and-constraints.html
[model scoping]: https://Diesel.org/master/manual/scopes.html
[Content Security Policy]: https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP
[Cross-Site Scripting]: https://developer.mozilla.org/en-US/docs/Glossary/Cross-site_scripting
[crossOriginResourcePolicy]: https://www.npmjs.com/package/helmet
[http://localhost:8000/hello/world]: http://localhost:8000/hello/world
[http://localhost:8000/not-found]: http://localhost:8000/not-found
[http://localhost:8000/api/set-token-cookie]: http://localhost:8000/api/set-token-cookie
[http://localhost:8000/api/restore-user]: http://localhost:8000/api/restore-user
[http://localhost:8000/api/require-auth]: http://localhost:8000/api/require-auth
[http://localhost:8000/api/session]: http://localhost:8000/api/session
[http://localhost:8000/api/csrf/restore]: http://localhost:8000/api/csrf/restore
